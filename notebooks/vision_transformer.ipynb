{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55730ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07698bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTokenizer(nn.Module):\n",
    "    def __init__(self, img_size, n_channels, patch_size, latent_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.conv2d_patch_tokenizer = nn.Conv2d(in_channels=n_channels, out_channels=latent_dim, kernel_size=(patch_size, patch_size), stride=patch_size, padding=0)\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        patches = self.conv2d_patch_tokenizer(img) # Shape (batch_size, latent_dim, patch_size, patch_size)\n",
    "        flattened_patches = self.flatten(patches) # Shape (batch_size, latent_dim, num_patches)\n",
    "        \n",
    "        return flattened_patches.permute(0, 2, 1) # Shape (batch_size, num_patches, latent_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48ad39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 196, 384])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "n_channels = 3\n",
    "latent_dim = 384\n",
    "\n",
    "img = torch.rand(batch_size, n_channels, img_height, img_width)\n",
    "\n",
    "patch_tokenizer = PatchTokenizer(224, 3, 16, latent_dim)\n",
    "\n",
    "patches = patch_tokenizer(img)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17bfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_token = nn.Parameter(torch.randn(batch_size, 1, latent_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcffb1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 197, 384])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_with_class_token = torch.cat((class_token, patches), dim=1)\n",
    "tensor_with_class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ce7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 197, 384])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnable_pos_embedding = nn.Parameter(torch.randn(batch_size, 197, latent_dim), requires_grad=True)\n",
    "tensor_with_pos_embedding = tensor_with_class_token + learnable_pos_embedding\n",
    "tensor_with_pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(key: torch.Tensor, query: torch.Tensor, value: torch.Tensor, mask=None):\n",
    "    d_k = key.shape(-1)\n",
    "    scaled_dot_product = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    attention = torch.nn.functional.softmax(scaled_dot_product, dim=-1)\n",
    "    values = torch.matmul(attention, values)\n",
    "\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.latent_dim = latent_dim         \n",
    "        self.num_heads = num_heads      \n",
    "        self.head_dim = latent_dim // num_heads  \n",
    "\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3 * latent_dim)\n",
    "        self.linear_layer = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=latent_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        qkv = self.qkv_layer(x) # Shape: (batch, seq_len, 3 * latent_dim)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # reshape into (batch, seq_len, num_heads, 3 * head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # Rearrange to (batch, num_heads, seq_len, 3 * head_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Split the last dimension into q, k, v (each get last dimension of head_dim) (batch, num_heads, seq_len, head_dim)\n",
    "        values, attention = scaled_dot_product_attention(q, k, v, mask) # Apply scaled dot product attention to get outputs (contextualized values) and attention weights\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim) # Merge the heads (concatenate the last head_dim axis)\n",
    "        out = self.linear_layer(values) # Final linear projection to match latent_dim\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6df47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_blocks, num_attention_heads, latent_dim):\n",
    "        super.__init__()\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_base_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
